{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "# import PyTorch Neural Network module\n",
    "import torch.nn as nn\n",
    "#random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các hàm Sigmoid Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(value): # hàm sigmoid activation\n",
    "    sig = 1 / (1 + torch.exp(-value))\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(value): # đạo hàm của hàm sigmoid activation\n",
    "    d = value * (1 - value)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class về Foward Feed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    # initialization function\n",
    "    def __init__(self):\n",
    "        # init function of base class\n",
    "        super(FFNN, self).__init__()\n",
    "\n",
    "        # corresponding size of each layer\n",
    "        self.inputlayer = 3 # number of perceptrons at input layer\n",
    "        self.hiddenlayer = 12 # number of perceptrons at hidden layer\n",
    "        self.outputlayer = 1 # number of perceptrons at output layer\n",
    "\n",
    "        # random weights from a normal distribution\n",
    "        self.W1 = torch.randn(self.inputlayer, self.hiddenlayer)   # 3 X 12 tensor\n",
    "        self.W2 = torch.randn(self.hiddenlayer, self.hiddenlayer)  # 12 X 12 tensor\n",
    "        self.W3 = torch.randn(self.hiddenlayer, self.outputlayer)  # 12 X 1 tensor\n",
    "\n",
    "        self.z = None # processing result between input layer and 1st hidden layer\n",
    "        self.a = None # sigmoid activation result of z\n",
    "\n",
    "        self.z_activation = None # save sigmoid activation result of z\n",
    "        self.z_activation_derivative = None # save the gradient descent sigmoid activation result of z\n",
    "\n",
    "\n",
    "        self.z2 = None # processing result between 1s hidden layer and 2nd hidden layer\n",
    "        self.a2 = None # sigmoid activation result of z2\n",
    "\n",
    "        self.z3 = None # processing result between 1s hidden layer and 2nd hidden layer\n",
    "        self.a3 = None # sigmoid activation result of z3\n",
    "\n",
    "        self.w3_error = None # Error value between correct result and predicted result\n",
    "        self.w3_delta = None # result of error * derivative of activation function of ai result (i = i-th layer) (i=3)\n",
    "\n",
    "        self.w2_error = None # Error value between correct result and predicted result\n",
    "        self.w2_delta = None # result of error * derivative of activation function of ai result (i = 2)\n",
    "\n",
    "        self.w1_error = None # Error value between correct result and predicted result\n",
    "        self.w1_delta = None # result of error * derivative of activation function of ai result (i = 1)\n",
    "\n",
    "        # a = sigmoid(z); z = x.T * W       \n",
    "\n",
    "    # activation function using sigmoid\n",
    "    def activation(self, z):\n",
    "        self.z_activation = sigmoid(z)\n",
    "        return self.z_activation\n",
    "\n",
    "    # derivative of activation function\n",
    "    def activation_derivative(self, z):\n",
    "        self.z_activation_derivative = sigmoid_derivative(z)\n",
    "        return self.z_activation_derivative\n",
    "\n",
    "    def forward(self, X):\n",
    "        # multiply input X and weights W1 from input layer to 1st hidden layer\n",
    "        self.z = torch.matmul(X, self.W1)\n",
    "        self.a = self.activation(self.z)  # activation function\n",
    "\n",
    "        # multiply current tensor and weights W2 from 1st hidden layer to 2nd hidden layer\n",
    "        self.z2 = torch.matmul(self.a, self.W2)\n",
    "        self.a2 = self.activation(self.z2)  # activation function\n",
    "\n",
    "        # multiply current tensor and weights W3 from 2nd hidden layer to output layer\n",
    "        self.z3 = torch.matmul(self.a2, self.W3)\n",
    "        self.a3 = self.activation(self.z3)  # activation function\n",
    "        return self.a3\n",
    "\n",
    "    def backward(self, X, y, res, rate):\n",
    "        # W3 adjustment rate\n",
    "        self.w3_error = y - res  # error in output\n",
    "        self.w3_delta = self.w3_error * self.activation_derivative(res)  # derivative of activation to error\n",
    "\n",
    "        # W2 adjustment rate\n",
    "        self.w2_error = torch.matmul(self.w3_delta, torch.t(self.W3))\n",
    "        self.w2_delta = self.w2_error * self.activation_derivative(self.a2)\n",
    "\n",
    "        # W1 adjustment rate\n",
    "        self.w1_error = torch.matmul(self.w2_delta, torch.t(self.W2))\n",
    "        self.w1_delta = self.w1_error * self.activation_derivative(self.a)\n",
    "\n",
    "        # update weights from delta of error and learning rate\n",
    "        self.W1 += torch.matmul(torch.t(X), self.w1_delta) * rate\n",
    "        self.W2 += torch.matmul(torch.t(self.a2), self.w2_delta) * rate\n",
    "        self.W3 += torch.matmul(torch.t(self.a3), self.w3_delta) * rate\n",
    "\n",
    "    # training function with learning rate parameter\n",
    "    def train(self, X, y, rate):\n",
    "        # forward + backward pass for training\n",
    "        result = self.forward(X)\n",
    "        self.backward(X, y, result, rate)\n",
    "\n",
    "    # predict function\n",
    "    def predict(self, x_predict):\n",
    "        print(\"Predict data based on trained weights: \")\n",
    "        print(\"Input: \\n\" + str(x_predict))\n",
    "        print(\"Output: \\n\" + str(self.forward(x_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveModel(model, name):\n",
    "   torch.save(model, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadModel(name):\n",
    "   model = torch.load(name)\n",
    "   return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chạy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khởi tạo dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('data.csv', usecols=[0,1,2]) # Read first 3 variable columns in dataset\n",
    "x_train = x_train.values.tolist() # Convert dataframe to list\n",
    "\n",
    "y_train = pd.read_csv('data.csv', usecols=[3]) # Read result columns in dataset\n",
    "y_train = y_train.values.tolist() # Convert dataframe to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "các dữ liệu về x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.5, 39.3, 45.1], [17.2, 45.9, 69.3], [151.5, 41.3, 58.5], [180.8, 10.8, 58.4], [8.7, 48.9, 75.0], [57.5, 32.8, 23.5], [120.2, 19.6, 11.6], [8.6, 2.1, 1.0], [199.8, 2.6, 21.2], [66.1, 5.8, 24.2], [214.7, 24.0, 4.0], [23.8, 35.1, 65.9], [97.5, 7.6, 7.2], [204.1, 32.9, 46.0], [195.4, 47.7, 52.9], [67.8, 36.6, 114.0], [281.4, 39.6, 55.8], [69.2, 20.5, 18.3], [147.3, 23.9, 19.1], [218.4, 27.7, 53.4], [237.4, 5.1, 23.5], [13.2, 15.9, 49.6], [228.3, 16.9, 26.2], [62.3, 12.6, 18.3], [262.9, 3.5, 19.5], [142.9, 29.3, 12.6], [240.1, 16.7, 22.9], [248.8, 27.1, 22.9], [70.6, 16.0, 40.8], [292.9, 28.3, 43.2], [112.9, 17.4, 38.6], [97.2, 1.5, 30.0], [265.6, 20.0, 0.3], [95.7, 1.4, 7.4], [290.7, 4.1, 8.5], [266.9, 43.8, 5.0], [74.7, 49.4, 45.7], [43.1, 26.7, 35.1], [228.0, 37.7, 32.0], [202.5, 22.3, 31.6], [177.0, 33.4, 38.7], [293.6, 27.7, 1.8], [206.9, 8.4, 26.4], [25.1, 25.7, 43.3], [175.1, 22.5, 31.5], [89.7, 9.9, 35.7], [239.9, 41.5, 18.5], [227.2, 15.8, 49.9], [66.9, 11.7, 36.8], [199.8, 3.1, 34.6], [100.4, 9.6, 3.6], [216.4, 41.7, 39.6], [182.6, 46.2, 58.7], [262.7, 28.8, 15.9], [198.9, 49.4, 60.0], [7.3, 28.1, 41.4], [136.2, 19.2, 16.6], [210.8, 49.6, 37.7], [210.7, 29.5, 9.3], [53.5, 2.0, 21.4], [261.3, 42.7, 54.7], [239.3, 15.5, 27.3], [102.7, 29.6, 8.4], [131.1, 42.8, 28.9], [69.0, 9.3, 0.9], [31.5, 24.6, 2.2], [139.3, 14.5, 10.2], [237.4, 27.5, 11.0], [216.8, 43.9, 27.2], [199.1, 30.6, 38.7], [109.8, 14.3, 31.7], [26.8, 33.0, 19.3], [129.4, 5.7, 31.3], [213.4, 24.6, 13.1], [16.9, 43.7, 89.4], [27.5, 1.6, 20.7], [120.5, 28.5, 14.2], [5.4, 29.9, 9.4], [116.0, 7.7, 23.1], [76.4, 26.7, 22.3], [239.8, 4.1, 36.9], [75.3, 20.3, 32.5], [68.4, 44.5, 35.6], [213.5, 43.0, 33.8], [193.2, 18.4, 65.7], [76.3, 27.5, 16.0], [110.7, 40.6, 63.2], [88.3, 25.5, 73.4], [109.8, 47.8, 51.4], [134.3, 4.9, 9.3], [28.6, 1.5, 33.0], [217.7, 33.5, 59.0], [250.9, 36.5, 72.3], [107.4, 14.0, 10.9], [163.3, 31.6, 52.9], [197.6, 3.5, 5.9], [184.9, 21.0, 22.0], [289.7, 42.3, 51.2], [135.2, 41.7, 45.9], [222.4, 4.3, 49.8], [296.4, 36.3, 100.9], [280.2, 10.1, 21.4], [187.9, 17.2, 17.9], [238.2, 34.3, 5.3], [137.9, 46.4, 59.0], [25.0, 11.0, 29.7], [90.4, 0.3, 23.2], [13.1, 0.4, 25.6], [255.4, 26.9, 5.5], [225.8, 8.2, 56.5], [241.7, 38.0, 23.2], [175.7, 15.4, 2.4], [209.6, 20.6, 10.7], [78.2, 46.8, 34.5], [75.1, 35.0, 52.7], [139.2, 14.3, 25.6], [76.4, 0.8, 14.8], [125.7, 36.9, 79.2], [19.4, 16.0, 22.3], [141.3, 26.8, 46.2], [18.8, 21.7, 50.4], [224.0, 2.4, 15.6], [123.1, 34.6, 12.4], [229.5, 32.3, 74.2], [87.2, 11.8, 25.9], [7.8, 38.9, 50.6], [80.2, 0.0, 9.2], [220.3, 49.0, 3.2], [59.6, 12.0, 43.1], [0.7, 39.6, 8.7], [265.2, 2.9, 43.0], [8.4, 27.2, 2.1], [219.8, 33.5, 45.1], [36.9, 38.6, 65.6], [48.3, 47.0, 8.5], [25.6, 39.0, 9.3], [273.7, 28.9, 59.7], [43.0, 25.9, 20.5], [184.9, 43.9, 1.7], [73.4, 17.0, 12.9], [193.7, 35.4, 75.6], [220.5, 33.2, 37.9], [104.6, 5.7, 34.4], [96.2, 14.8, 38.9], [140.3, 1.9, 9.0], [240.1, 7.3, 8.7], [243.2, 49.0, 44.3], [38.0, 40.3, 11.9], [44.7, 25.8, 20.6], [280.7, 13.9, 37.0], [121.0, 8.4, 48.7], [197.6, 23.3, 14.2], [171.3, 39.7, 37.7], [187.8, 21.1, 9.5], [4.1, 11.6, 5.7], [93.9, 43.5, 50.5], [149.8, 1.3, 24.3], [11.7, 36.9, 45.2], [131.7, 18.4, 34.6], [172.5, 18.1, 30.7], [85.7, 35.8, 49.3], [188.4, 18.1, 25.6], [163.5, 36.8, 7.4], [117.2, 14.7, 5.4], [234.5, 3.4, 84.8], [17.9, 37.6, 21.6], [206.8, 5.2, 19.4], [215.4, 23.6, 57.6], [284.3, 10.6, 6.4], [50.0, 11.6, 18.4], [164.5, 20.9, 47.4], [19.6, 20.1, 17.0], [168.4, 7.1, 12.8], [222.4, 3.4, 13.1], [276.9, 48.9, 41.8], [248.4, 30.2, 20.3], [170.2, 7.8, 35.2], [276.7, 2.3, 23.7], [165.6, 10.0, 17.6], [156.6, 2.6, 8.3], [218.5, 5.4, 27.4], [56.2, 5.7, 29.7], [287.6, 43.0, 71.8], [253.8, 21.3, 30.0], [205.0, 45.1, 19.6], [139.5, 2.1, 26.6], [191.1, 28.7, 18.2], [286.0, 13.9, 3.7], [18.7, 12.1, 23.4], [39.5, 41.1, 5.8], [75.5, 10.8, 6.0], [17.2, 4.1, 31.6], [166.8, 42.0, 3.6], [149.7, 35.6, 6.0], [38.2, 3.7, 13.8], [94.2, 4.9, 8.1], [177.0, 9.3, 6.4], [283.6, 42.0, 66.2], [232.1, 8.6, 8.7]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.4], [12.0], [16.5], [17.9], [7.2], [11.8], [13.2], [4.8], [15.6], [12.6], [17.4], [9.2], [13.7], [19.0], [22.4], [12.5], [24.4], [11.3], [14.6], [18.0], [17.5], [5.6], [20.5], [9.7], [17.0], [15.0], [20.9], [18.9], [10.5], [21.4], [11.9], [13.2], [17.4], [11.9], [17.8], [25.4], [14.7], [10.1], [21.5], [16.6], [17.1], [20.7], [17.9], [8.5], [16.1], [10.6], [23.2], [19.8], [9.7], [16.4], [10.7], [22.6], [21.2], [20.2], [23.7], [5.5], [13.2], [23.8], [18.4], [8.1], [24.2], [20.7], [14.0], [16.0], [11.3], [11.0], [13.4], [18.9], [22.3], [18.3], [12.4], [8.8], [11.0], [17.0], [8.7], [6.9], [14.2], [5.3], [11.0], [11.8], [17.3], [11.3], [13.6], [21.7], [20.2], [12.0], [16.0], [12.9], [16.7], [14.0], [7.3], [19.4], [22.2], [11.5], [16.9], [16.7], [20.5], [25.4], [17.2], [16.7], [23.8], [19.8], [19.7], [20.7], [15.0], [7.2], [12.0], [5.3], [19.8], [18.4], [21.8], [17.1], [20.9], [14.6], [12.6], [12.2], [9.4], [15.9], [6.6], [15.5], [7.0], [16.6], [15.2], [19.7], [10.6], [6.6], [11.9], [24.7], [9.7], [1.6], [17.7], [5.7], [19.6], [10.8], [11.6], [9.5], [20.8], [9.6], [20.7], [10.9], [19.2], [20.1], [10.4], [12.3], [10.3], [18.2], [25.4], [10.9], [10.1], [16.1], [11.6], [16.6], [16.0], [20.6], [3.2], [15.3], [10.1], [7.3], [12.9], [16.4], [13.3], [19.9], [18.0], [11.9], [16.9], [8.0], [17.2], [17.1], [20.0], [8.4], [17.5], [7.6], [16.7], [16.5], [27.0], [20.2], [16.7], [16.8], [17.6], [15.5], [17.2], [8.7], [26.2], [17.6], [22.6], [10.3], [17.3], [20.9], [6.7], [10.8], [11.9], [5.9], [19.6], [17.3], [7.6], [14.0], [14.8], [25.5], [18.4]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(x_train[:60], dtype=torch.float)  # 60 X 3 tensor\n",
    "y = torch.tensor(y_train[:60], dtype=torch.float)  # 60 X 1 tensor\n",
    "\n",
    "# scale units by max value\n",
    "X_max, _ = torch.max(X, 0)\n",
    "X = torch.div(X, X_max)\n",
    "y = y / 100  # for max test score is 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dữ liệu dùng để test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample input x for predicting\n",
    "x_predict = torch.tensor(([38.2, 3.7, 13.8]), dtype=torch.float)  # 3 X 1 tensor\n",
    "# correct one is 7.6\n",
    "\n",
    "# scale input x by max value\n",
    "x_predict_max, _ = torch.max(x_predict, 0)\n",
    "x_predict = torch.div(x_predict, x_predict_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chạy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 0.022732950747013092\n",
      "#1 Loss: 0.022660115733742714\n",
      "#2 Loss: 0.02258482575416565\n",
      "#3 Loss: 0.022506969049572945\n",
      "#4 Loss: 0.022426428273320198\n",
      "#5 Loss: 0.022343087941408157\n",
      "#6 Loss: 0.022256817668676376\n",
      "#7 Loss: 0.02216748334467411\n",
      "#8 Loss: 0.022074948996305466\n",
      "#9 Loss: 0.021979061886668205\n",
      "#10 Loss: 0.021879667416214943\n",
      "#11 Loss: 0.021776605397462845\n",
      "#12 Loss: 0.021669697016477585\n",
      "#13 Loss: 0.021558769047260284\n",
      "#14 Loss: 0.021443627774715424\n",
      "#15 Loss: 0.021324072033166885\n",
      "#16 Loss: 0.021199895069003105\n",
      "#17 Loss: 0.021070880815386772\n",
      "#18 Loss: 0.020936794579029083\n",
      "#19 Loss: 0.020797397941350937\n",
      "#20 Loss: 0.020652439445257187\n",
      "#21 Loss: 0.020501647144556046\n",
      "#22 Loss: 0.020344754680991173\n",
      "#23 Loss: 0.02018147148191929\n",
      "#24 Loss: 0.020011497661471367\n",
      "#25 Loss: 0.019834520295262337\n",
      "#26 Loss: 0.01965022087097168\n",
      "#27 Loss: 0.019458262249827385\n",
      "#28 Loss: 0.019258299842476845\n",
      "#29 Loss: 0.019049979746341705\n",
      "#30 Loss: 0.018832935020327568\n",
      "#31 Loss: 0.018606793135404587\n",
      "#32 Loss: 0.018371175974607468\n",
      "#33 Loss: 0.018125705420970917\n",
      "#34 Loss: 0.017869988456368446\n",
      "#35 Loss: 0.017603648826479912\n",
      "#36 Loss: 0.01732630282640457\n",
      "#37 Loss: 0.017037587240338326\n",
      "#38 Loss: 0.016737140715122223\n",
      "#39 Loss: 0.0164246316999197\n",
      "#40 Loss: 0.016099752858281136\n",
      "#41 Loss: 0.015762224793434143\n",
      "#42 Loss: 0.015411823987960815\n",
      "#43 Loss: 0.015048367902636528\n",
      "#44 Loss: 0.014671748504042625\n",
      "#45 Loss: 0.0142819257453084\n",
      "#46 Loss: 0.01387895829975605\n",
      "#47 Loss: 0.013463000766932964\n",
      "#48 Loss: 0.013034330680966377\n",
      "#49 Loss: 0.012593363411724567\n",
      "#50 Loss: 0.012140659615397453\n",
      "#51 Loss: 0.011676955036818981\n",
      "#52 Loss: 0.0112031614407897\n",
      "#53 Loss: 0.010720388032495975\n",
      "#54 Loss: 0.010229948908090591\n",
      "#55 Loss: 0.009733377024531364\n",
      "#56 Loss: 0.009232417680323124\n",
      "#57 Loss: 0.008729027584195137\n",
      "#58 Loss: 0.008225376717746258\n",
      "#59 Loss: 0.007723803631961346\n",
      "#60 Loss: 0.0072268154472112656\n",
      "#61 Loss: 0.006737035699188709\n",
      "#62 Loss: 0.006257152650505304\n",
      "#63 Loss: 0.005789875518530607\n",
      "#64 Loss: 0.005337860900908709\n",
      "#65 Loss: 0.004903641529381275\n",
      "#66 Loss: 0.004489564336836338\n",
      "#67 Loss: 0.0040977089665830135\n",
      "#68 Loss: 0.003729831427335739\n",
      "#69 Loss: 0.0033872986678034067\n",
      "#70 Loss: 0.003071062033995986\n",
      "#71 Loss: 0.00278162001632154\n",
      "#72 Loss: 0.0025190236046910286\n",
      "#73 Loss: 0.002282888162881136\n",
      "#74 Loss: 0.002072424627840519\n",
      "#75 Loss: 0.0018864923622459173\n",
      "#76 Loss: 0.001723656663671136\n",
      "#77 Loss: 0.001582263270393014\n",
      "#78 Loss: 0.0014605039032176137\n",
      "#79 Loss: 0.0013564886758103967\n",
      "#80 Loss: 0.001268310472369194\n",
      "#81 Loss: 0.0011940988479182124\n",
      "#82 Loss: 0.0011320642661303282\n",
      "#83 Loss: 0.0010805336060002446\n",
      "#84 Loss: 0.0010379693703725934\n",
      "#85 Loss: 0.001002988894470036\n",
      "#86 Loss: 0.0009743634727783501\n",
      "#87 Loss: 0.0009510178351774812\n",
      "#88 Loss: 0.0009320242679677904\n",
      "#89 Loss: 0.0009165909141302109\n",
      "#90 Loss: 0.0009040485601872206\n",
      "#91 Loss: 0.0008938380633480847\n",
      "#92 Loss: 0.0008854964980855584\n",
      "#93 Loss: 0.0008786430698819458\n",
      "#94 Loss: 0.0008729667752049863\n",
      "#95 Loss: 0.0008682164479978383\n",
      "#96 Loss: 0.0008641903987154365\n",
      "#97 Loss: 0.0008607266936451197\n",
      "#98 Loss: 0.0008576959371566772\n",
      "#99 Loss: 0.0008549984777346253\n",
      "#100 Loss: 0.0008525530574843287\n",
      "#101 Loss: 0.0008502986747771502\n",
      "#102 Loss: 0.0008481869590468705\n",
      "#103 Loss: 0.0008461801917292178\n",
      "#104 Loss: 0.0008442510152235627\n",
      "#105 Loss: 0.0008423764957115054\n",
      "#106 Loss: 0.0008405414992012084\n",
      "#107 Loss: 0.0008387334528379142\n",
      "#108 Loss: 0.0008369436836801469\n",
      "#109 Loss: 0.0008351653814315796\n",
      "#110 Loss: 0.0008333947625942528\n",
      "#111 Loss: 0.0008316279854625463\n",
      "#112 Loss: 0.0008298633038066328\n",
      "#113 Loss: 0.0008280996116809547\n",
      "#114 Loss: 0.0008263355703093112\n",
      "#115 Loss: 0.0008245716453529894\n",
      "#116 Loss: 0.0008228070219047368\n",
      "#117 Loss: 0.0008210432133637369\n",
      "#118 Loss: 0.0008192795212380588\n",
      "#119 Loss: 0.0008175168768502772\n",
      "#120 Loss: 0.0008157553384080529\n",
      "#121 Loss: 0.0008139952551573515\n",
      "#122 Loss: 0.000812237907666713\n",
      "#123 Loss: 0.0008104833541437984\n",
      "#124 Loss: 0.0008087321766652167\n",
      "#125 Loss: 0.0008069844334386289\n",
      "#126 Loss: 0.0008052409975789487\n",
      "#127 Loss: 0.0008035021601244807\n",
      "#128 Loss: 0.000801767804659903\n",
      "#129 Loss: 0.0008000385132618248\n",
      "#130 Loss: 0.0007983145769685507\n",
      "#131 Loss: 0.0007965961121954024\n",
      "#132 Loss: 0.0007948836428113282\n",
      "#133 Loss: 0.0007931770524010062\n",
      "#134 Loss: 0.000791476690210402\n",
      "#135 Loss: 0.0007897821487858891\n",
      "#136 Loss: 0.000788094534073025\n",
      "#137 Loss: 0.0007864130311645567\n",
      "#138 Loss: 0.000784738571383059\n",
      "#139 Loss: 0.0007830699905753136\n",
      "#140 Loss: 0.0007814086275175214\n",
      "#141 Loss: 0.000779753434471786\n",
      "#142 Loss: 0.0007781057502143085\n",
      "#143 Loss: 0.0007764642941765487\n",
      "#144 Loss: 0.0007748297066427767\n",
      "#145 Loss: 0.000773202336858958\n",
      "#146 Loss: 0.000771581195294857\n",
      "#147 Loss: 0.0007699674461036921\n",
      "#148 Loss: 0.0007683608564548194\n",
      "#149 Loss: 0.0007667603786103427\n",
      "#150 Loss: 0.0007651668856851757\n",
      "#151 Loss: 0.0007635802030563354\n",
      "#152 Loss: 0.0007620005053468049\n",
      "#153 Loss: 0.0007604275015182793\n",
      "#154 Loss: 0.0007588613661937416\n",
      "#155 Loss: 0.0007573016337119043\n",
      "#156 Loss: 0.0007557488861493766\n",
      "#157 Loss: 0.0007542029488831758\n",
      "#158 Loss: 0.0007526632398366928\n",
      "#159 Loss: 0.0007511302828788757\n",
      "#160 Loss: 0.0007496044272556901\n",
      "#161 Loss: 0.0007480847416445613\n",
      "#162 Loss: 0.0007465715752914548\n",
      "#163 Loss: 0.0007450651610270143\n",
      "#164 Loss: 0.0007435654406435788\n",
      "#165 Loss: 0.0007420715410262346\n",
      "#166 Loss: 0.0007405845681205392\n",
      "#167 Loss: 0.0007391039980575442\n",
      "#168 Loss: 0.0007376295397989452\n",
      "#169 Loss: 0.0007361614261753857\n",
      "#170 Loss: 0.0007346997736021876\n",
      "#171 Loss: 0.0007332442328333855\n",
      "#172 Loss: 0.0007317950949072838\n",
      "#173 Loss: 0.0007303523598238826\n",
      "#174 Loss: 0.0007289156201295555\n",
      "#175 Loss: 0.0007274848176166415\n",
      "#176 Loss: 0.0007260599522851408\n",
      "#177 Loss: 0.0007246416644193232\n",
      "#178 Loss: 0.0007232290809042752\n",
      "#179 Loss: 0.0007218232494778931\n",
      "#180 Loss: 0.0007204227731563151\n",
      "#181 Loss: 0.0007190282340161502\n",
      "#182 Loss: 0.0007176396902650595\n",
      "#183 Loss: 0.00071625696728006\n",
      "#184 Loss: 0.0007148805889301002\n",
      "#185 Loss: 0.0007135095074772835\n",
      "#186 Loss: 0.000712144304998219\n",
      "#187 Loss: 0.0007107850979082286\n",
      "#188 Loss: 0.0007094317697919905\n",
      "#189 Loss: 0.0007080836803652346\n",
      "#190 Loss: 0.000706741469912231\n",
      "#191 Loss: 0.0007054051966406405\n",
      "#192 Loss: 0.0007040741620585322\n",
      "#193 Loss: 0.0007027489482425153\n",
      "#194 Loss: 0.0007014292059466243\n",
      "#195 Loss: 0.0007001151680015028\n",
      "#196 Loss: 0.0006988066015765071\n",
      "#197 Loss: 0.00069750304101035\n",
      "#198 Loss: 0.000696205475833267\n",
      "#199 Loss: 0.0006949132075533271\n",
      "#200 Loss: 0.0006936263525858521\n",
      "#201 Loss: 0.0006923449109308422\n",
      "#202 Loss: 0.0006910685333423316\n",
      "#203 Loss: 0.000689797627273947\n",
      "#204 Loss: 0.0006885319598950446\n",
      "#205 Loss: 0.0006872716476209462\n",
      "#206 Loss: 0.0006860166322439909\n",
      "#207 Loss: 0.000684766739141196\n",
      "#208 Loss: 0.0006835220265202224\n",
      "#209 Loss: 0.0006822823197580874\n",
      "#210 Loss: 0.0006810477352701128\n",
      "#211 Loss: 0.0006798185058869421\n",
      "#212 Loss: 0.0006785939913243055\n",
      "#213 Loss: 0.000677374773658812\n",
      "#214 Loss: 0.0006761607364751399\n",
      "#215 Loss: 0.0006749511230736971\n",
      "#216 Loss: 0.0006737470394000411\n",
      "#217 Loss: 0.0006725475541315973\n",
      "#218 Loss: 0.0006713531329296529\n",
      "#219 Loss: 0.0006701637175865471\n",
      "#220 Loss: 0.0006689788424409926\n",
      "#221 Loss: 0.0006677989149466157\n",
      "#222 Loss: 0.0006666239351034164\n",
      "#223 Loss: 0.0006654538447037339\n",
      "#224 Loss: 0.0006642882362939417\n",
      "#225 Loss: 0.0006631275755353272\n",
      "#226 Loss: 0.0006619714549742639\n",
      "#227 Loss: 0.0006608201656490564\n",
      "#228 Loss: 0.0006596734165214002\n",
      "#229 Loss: 0.000658531382214278\n",
      "#230 Loss: 0.0006573938298970461\n",
      "#231 Loss: 0.00065626110881567\n",
      "#232 Loss: 0.0006551325204782188\n",
      "#233 Loss: 0.0006540091708302498\n",
      "#234 Loss: 0.0006528897793032229\n",
      "#235 Loss: 0.0006517752772197127\n",
      "#236 Loss: 0.0006506647914648056\n",
      "#237 Loss: 0.0006495593697763979\n",
      "#238 Loss: 0.0006484580808319151\n",
      "#239 Loss: 0.0006473611574620008\n",
      "#240 Loss: 0.0006462689489126205\n",
      "#241 Loss: 0.0006451804074458778\n",
      "#242 Loss: 0.0006440968136303127\n",
      "#243 Loss: 0.0006430172943510115\n",
      "#244 Loss: 0.0006419421406462789\n",
      "#245 Loss: 0.000640871177893132\n",
      "#246 Loss: 0.0006398045807145536\n",
      "#247 Loss: 0.0006387421162799001\n",
      "#248 Loss: 0.0006376841920427978\n",
      "#249 Loss: 0.0006366300513036549\n",
      "#250 Loss: 0.0006355802179314196\n",
      "#251 Loss: 0.0006345346919260919\n",
      "#252 Loss: 0.0006334931240417063\n",
      "#253 Loss: 0.0006324556889012456\n",
      "#254 Loss: 0.0006314223865047097\n",
      "#255 Loss: 0.000630393042229116\n",
      "#256 Loss: 0.000629367888905108\n",
      "#257 Loss: 0.0006283466937020421\n",
      "#258 Loss: 0.0006273295730352402\n",
      "#259 Loss: 0.0006263164104893804\n",
      "#260 Loss: 0.0006253070896491408\n",
      "#261 Loss: 0.0006243017269298434\n",
      "#262 Loss: 0.00062330043874681\n",
      "#263 Loss: 0.0006223029340617359\n",
      "#264 Loss: 0.0006213096203282475\n",
      "#265 Loss: 0.0006203199154697359\n",
      "#266 Loss: 0.0006193342269398272\n",
      "#267 Loss: 0.0006183522054925561\n",
      "#268 Loss: 0.0006173741421662271\n",
      "#269 Loss: 0.0006163996295072138\n",
      "#270 Loss: 0.0006154289003461599\n",
      "#271 Loss: 0.0006144623621366918\n",
      "#272 Loss: 0.0006134990253485739\n",
      "#273 Loss: 0.0006125396466813982\n",
      "#274 Loss: 0.0006115837604738772\n",
      "#275 Loss: 0.0006106319488026202\n",
      "#276 Loss: 0.0006096838042140007\n",
      "#277 Loss: 0.0006087390938773751\n",
      "#278 Loss: 0.0006077982252463698\n",
      "#279 Loss: 0.0006068607326596975\n",
      "#280 Loss: 0.0006059268489480019\n",
      "#281 Loss: 0.0006049966905266047\n",
      "#282 Loss: 0.0006040699663572013\n",
      "#283 Loss: 0.000603146618232131\n",
      "#284 Loss: 0.0006022272282280028\n",
      "#285 Loss: 0.0006013112724758685\n",
      "#286 Loss: 0.0006003986345604062\n",
      "#287 Loss: 0.000599489314481616\n",
      "#288 Loss: 0.0005985833704471588\n",
      "#289 Loss: 0.000597681209910661\n",
      "#290 Loss: 0.0005967827746644616\n",
      "#291 Loss: 0.0005958871333859861\n",
      "#292 Loss: 0.0005949950427748263\n",
      "#293 Loss: 0.0005941065028309822\n",
      "#294 Loss: 0.0005932209896855056\n",
      "#295 Loss: 0.0005923391436226666\n",
      "#296 Loss: 0.0005914606153964996\n",
      "#297 Loss: 0.0005905852885916829\n",
      "#298 Loss: 0.0005897132214158773\n",
      "#299 Loss: 0.0005888444720767438\n",
      "#300 Loss: 0.0005879791569896042\n",
      "#301 Loss: 0.0005871168104931712\n",
      "#302 Loss: 0.0005862578982487321\n",
      "#303 Loss: 0.0005854022456333041\n",
      "#304 Loss: 0.0005845495616085827\n",
      "#305 Loss: 0.0005837000790052116\n",
      "#306 Loss: 0.0005828540306538343\n",
      "#307 Loss: 0.0005820107762701809\n",
      "#308 Loss: 0.0005811709561385214\n",
      "#309 Loss: 0.0005803342792205513\n",
      "#310 Loss: 0.0005795003962703049\n",
      "#311 Loss: 0.0005786698311567307\n",
      "#312 Loss: 0.0005778424092568457\n",
      "#313 Loss: 0.0005770178395323455\n",
      "#314 Loss: 0.0005761964130215347\n",
      "#315 Loss: 0.0005753780133090913\n",
      "#316 Loss: 0.00057456293143332\n",
      "#317 Loss: 0.0005737502942793071\n",
      "#318 Loss: 0.0005729408585466444\n",
      "#319 Loss: 0.0005721347406506538\n",
      "#320 Loss: 0.0005713312420994043\n",
      "#321 Loss: 0.0005705308285541832\n",
      "#322 Loss: 0.0005697335000149906\n",
      "#323 Loss: 0.0005689387326128781\n",
      "#324 Loss: 0.0005681473412550986\n",
      "#325 Loss: 0.0005673582782037556\n",
      "#326 Loss: 0.0005665723001584411\n",
      "#327 Loss: 0.0005657894071191549\n",
      "#328 Loss: 0.0005650093080475926\n",
      "#329 Loss: 0.0005642318283207715\n",
      "#330 Loss: 0.0005634572589769959\n",
      "#331 Loss: 0.0005626857746392488\n",
      "#332 Loss: 0.0005619169678539038\n",
      "#333 Loss: 0.0005611508968286216\n",
      "#334 Loss: 0.0005603876779787242\n",
      "#335 Loss: 0.000559627020265907\n",
      "#336 Loss: 0.0005588691565208137\n",
      "#337 Loss: 0.0005581142613664269\n",
      "#338 Loss: 0.0005573619273491204\n",
      "#339 Loss: 0.0005566123872995377\n",
      "#340 Loss: 0.000555865524802357\n",
      "#341 Loss: 0.0005551212816499174\n",
      "#342 Loss: 0.0005543797160498798\n",
      "#343 Loss: 0.0005536409444175661\n",
      "#344 Loss: 0.000552904442884028\n",
      "#345 Loss: 0.0005521709681488574\n",
      "#346 Loss: 0.000551439996343106\n",
      "#347 Loss: 0.0005507117020897567\n",
      "#348 Loss: 0.0005499860853888094\n",
      "#349 Loss: 0.000549262622371316\n",
      "#350 Loss: 0.0005485423025675118\n",
      "#351 Loss: 0.0005478243692778051\n",
      "#352 Loss: 0.0005471087642945349\n",
      "#353 Loss: 0.0005463958950713277\n",
      "#354 Loss: 0.0005456855869852006\n",
      "#355 Loss: 0.000544977723620832\n",
      "#356 Loss: 0.0005442723631858826\n",
      "#357 Loss: 0.0005435694474726915\n",
      "#358 Loss: 0.0005428693839348853\n",
      "#359 Loss: 0.0005421714158728719\n",
      "#360 Loss: 0.0005414758925326169\n",
      "#361 Loss: 0.0005407830467447639\n",
      "#362 Loss: 0.0005400926456786692\n",
      "#363 Loss: 0.0005394043982960284\n",
      "#364 Loss: 0.0005387189448811114\n",
      "#365 Loss: 0.0005380358197726309\n",
      "#366 Loss: 0.0005373547319322824\n",
      "#367 Loss: 0.0005366766708903015\n",
      "#368 Loss: 0.0005360004142858088\n",
      "#369 Loss: 0.0005353268934413791\n",
      "#370 Loss: 0.0005346554098650813\n",
      "#371 Loss: 0.0005339864874258637\n",
      "#372 Loss: 0.0005333200097084045\n",
      "#373 Loss: 0.0005326556856743991\n",
      "#374 Loss: 0.0005319935735315084\n",
      "#375 Loss: 0.0005313336150720716\n",
      "#376 Loss: 0.0005306763341650367\n",
      "#377 Loss: 0.0005300213233567774\n",
      "#378 Loss: 0.0005293682916089892\n",
      "#379 Loss: 0.0005287177627906203\n",
      "#380 Loss: 0.0005280693876557052\n",
      "#381 Loss: 0.0005274234572425485\n",
      "#382 Loss: 0.0005267795058898628\n",
      "#383 Loss: 0.0005261378828436136\n",
      "#384 Loss: 0.0005254982388578355\n",
      "#385 Loss: 0.0005248613306321204\n",
      "#386 Loss: 0.0005242262850515544\n",
      "#387 Loss: 0.0005235935095697641\n",
      "#388 Loss: 0.0005229627131484449\n",
      "#389 Loss: 0.0005223340704105794\n",
      "#390 Loss: 0.0005217077559791505\n",
      "#391 Loss: 0.0005210837116464972\n",
      "#392 Loss: 0.000520461646374315\n",
      "#393 Loss: 0.0005198417929932475\n",
      "#394 Loss: 0.000519223918672651\n",
      "#395 Loss: 0.0005186081398278475\n",
      "#396 Loss: 0.0005179948057048023\n",
      "#397 Loss: 0.0005173831596039236\n",
      "#398 Loss: 0.0005167737253941596\n",
      "#399 Loss: 0.0005161662120372057\n",
      "#400 Loss: 0.0005155609687790275\n",
      "#401 Loss: 0.000514957879204303\n",
      "#402 Loss: 0.0005143564776517451\n",
      "#403 Loss: 0.0005137572879903018\n",
      "#404 Loss: 0.0005131601938046515\n",
      "#405 Loss: 0.000512565195094794\n",
      "#406 Loss: 0.0005119720590300858\n",
      "#407 Loss: 0.0005113810766488314\n",
      "#408 Loss: 0.0005107920151203871\n",
      "#409 Loss: 0.0005102047580294311\n",
      "#410 Loss: 0.000509619596414268\n",
      "#411 Loss: 0.0005090365302748978\n",
      "#412 Loss: 0.0005084552685730159\n",
      "#413 Loss: 0.000507876044139266\n",
      "#414 Loss: 0.0005072987405583262\n",
      "#415 Loss: 0.0005067232996225357\n",
      "#416 Loss: 0.0005061498377472162\n",
      "#417 Loss: 0.0005055782967247069\n",
      "#418 Loss: 0.0005050086765550077\n",
      "#419 Loss: 0.0005044411518611014\n",
      "#420 Loss: 0.0005038750823587179\n",
      "#421 Loss: 0.000503311341162771\n",
      "#422 Loss: 0.0005027492879889905\n",
      "#423 Loss: 0.0005021888646297157\n",
      "#424 Loss: 0.0005016305949538946\n",
      "#425 Loss: 0.0005010744207538664\n",
      "#426 Loss: 0.000500519759953022\n",
      "#427 Loss: 0.0004999667289666831\n",
      "#428 Loss: 0.0004994157934561372\n",
      "#429 Loss: 0.0004988667787984014\n",
      "#430 Loss: 0.0004983193939551711\n",
      "#431 Loss: 0.0004977738135494292\n",
      "#432 Loss: 0.0004972300375811756\n",
      "#433 Loss: 0.0004966882988810539\n",
      "#434 Loss: 0.0004961481899954379\n",
      "#435 Loss: 0.0004956098273396492\n",
      "#436 Loss: 0.0004950732109136879\n",
      "#437 Loss: 0.0004945382825098932\n",
      "#438 Loss: 0.0004940052749589086\n",
      "#439 Loss: 0.0004934741300530732\n",
      "#440 Loss: 0.0004929443239234388\n",
      "#441 Loss: 0.0004924163222312927\n",
      "#442 Loss: 0.0004918903578072786\n",
      "#443 Loss: 0.0004913659649901092\n",
      "#444 Loss: 0.0004908433184027672\n",
      "#445 Loss: 0.0004903223016299307\n",
      "#446 Loss: 0.0004898031475022435\n",
      "#447 Loss: 0.00048928550677374\n",
      "#448 Loss: 0.000488769612275064\n",
      "#449 Loss: 0.00048825537669472396\n",
      "#450 Loss: 0.0004877427709288895\n",
      "#451 Loss: 0.0004872318240813911\n",
      "#452 Loss: 0.0004867226234637201\n",
      "#453 Loss: 0.0004862150235567242\n",
      "#454 Loss: 0.0004857090534642339\n",
      "#455 Loss: 0.0004852048878092319\n",
      "#456 Loss: 0.0004847019736189395\n",
      "#457 Loss: 0.00048420100938528776\n",
      "#458 Loss: 0.00048370155855081975\n",
      "#459 Loss: 0.00048320365021936595\n",
      "#460 Loss: 0.00048270754632540047\n",
      "#461 Loss: 0.00048221281031146646\n",
      "#462 Loss: 0.00048171982052735984\n",
      "#463 Loss: 0.0004812280531041324\n",
      "#464 Loss: 0.0004807381483260542\n",
      "#465 Loss: 0.00048024996067397296\n",
      "#466 Loss: 0.0004797631991095841\n",
      "#467 Loss: 0.0004792778636328876\n",
      "#468 Loss: 0.00047879424528218806\n",
      "#469 Loss: 0.00047831222764216363\n",
      "#470 Loss: 0.0004778316942974925\n",
      "#471 Loss: 0.0004773526161443442\n",
      "#472 Loss: 0.0004768749640788883\n",
      "#473 Loss: 0.00047639902913942933\n",
      "#474 Loss: 0.0004759244911838323\n",
      "#475 Loss: 0.000475451786769554\n",
      "#476 Loss: 0.0004749801300931722\n",
      "#477 Loss: 0.00047451036516577005\n",
      "#478 Loss: 0.000474041880806908\n",
      "#479 Loss: 0.0004735747934319079\n",
      "#480 Loss: 0.00047310939407907426\n",
      "#481 Loss: 0.0004726451588794589\n",
      "#482 Loss: 0.0004721827572211623\n",
      "#483 Loss: 0.0004717216652352363\n",
      "#484 Loss: 0.0004712618829216808\n",
      "#485 Loss: 0.00047080349759198725\n",
      "#486 Loss: 0.0004703467129729688\n",
      "#487 Loss: 0.0004698914708569646\n",
      "#488 Loss: 0.000469437800347805\n",
      "#489 Loss: 0.0004689850611612201\n",
      "#490 Loss: 0.0004685341555159539\n",
      "#491 Loss: 0.00046808438492007554\n",
      "#492 Loss: 0.00046763618593104184\n",
      "#493 Loss: 0.00046718926751054823\n",
      "#494 Loss: 0.0004667439206968993\n",
      "#495 Loss: 0.00046629985445179045\n",
      "#496 Loss: 0.0004658572725020349\n",
      "#497 Loss: 0.0004654158547054976\n",
      "#498 Loss: 0.0004649762122426182\n",
      "#499 Loss: 0.0004645376175176352\n",
      "#500 Loss: 0.0004641003324650228\n",
      "#501 Loss: 0.0004636647063307464\n",
      "#502 Loss: 0.00046323012793436646\n",
      "#503 Loss: 0.0004627971211448312\n",
      "#504 Loss: 0.0004623652494046837\n",
      "#505 Loss: 0.00046193497837521136\n",
      "#506 Loss: 0.0004615057841874659\n",
      "#507 Loss: 0.00046107807429507375\n",
      "#508 Loss: 0.0004606516449712217\n",
      "#509 Loss: 0.0004602265253197402\n",
      "#510 Loss: 0.00045980262802913785\n",
      "#511 Loss: 0.00045938012772239745\n",
      "#512 Loss: 0.0004589590535033494\n",
      "#513 Loss: 0.00045853896881453693\n",
      "#514 Loss: 0.0004581205721478909\n",
      "#515 Loss: 0.00045770322321914136\n",
      "#516 Loss: 0.0004572872130665928\n",
      "#517 Loss: 0.000456872396171093\n",
      "#518 Loss: 0.0004564589762594551\n",
      "#519 Loss: 0.0004560469533316791\n",
      "#520 Loss: 0.00045563580351881683\n",
      "#521 Loss: 0.0004552262253127992\n",
      "#522 Loss: 0.0004548175784293562\n",
      "#523 Loss: 0.00045441032852977514\n",
      "#524 Loss: 0.0004540045338217169\n",
      "#525 Loss: 0.0004535998741630465\n",
      "#526 Loss: 0.00045319629134610295\n",
      "#527 Loss: 0.00045279401820153\n",
      "#528 Loss: 0.0004523930256254971\n",
      "#529 Loss: 0.0004519932554103434\n",
      "#530 Loss: 0.000451594969490543\n",
      "#531 Loss: 0.00045119746937416494\n",
      "#532 Loss: 0.0004508012207224965\n",
      "#533 Loss: 0.00045040628174319863\n",
      "#534 Loss: 0.0004500125360209495\n",
      "#535 Loss: 0.0004496197507251054\n",
      "#536 Loss: 0.00044922850793227553\n",
      "#537 Loss: 0.00044883834198117256\n",
      "#538 Loss: 0.0004484492528717965\n",
      "#539 Loss: 0.00044806161895394325\n",
      "#540 Loss: 0.00044767477083951235\n",
      "#541 Loss: 0.0004472893488127738\n",
      "#542 Loss: 0.00044690482900477946\n",
      "#543 Loss: 0.00044652167707681656\n",
      "#544 Loss: 0.00044613974750973284\n",
      "#545 Loss: 0.00044575880747288465\n",
      "#546 Loss: 0.0004453790024854243\n",
      "#547 Loss: 0.00044500039075501263\n",
      "#548 Loss: 0.00044462314690463245\n",
      "#549 Loss: 0.00044424665975384414\n",
      "#550 Loss: 0.0004438713367562741\n",
      "#551 Loss: 0.00044349729432724416\n",
      "#552 Loss: 0.0004431243287399411\n",
      "#553 Loss: 0.0004427525564096868\n",
      "#554 Loss: 0.000442381773609668\n",
      "#555 Loss: 0.0004420122131705284\n",
      "#556 Loss: 0.0004416434676386416\n",
      "#557 Loss: 0.00044127614819444716\n",
      "#558 Loss: 0.0004409097309689969\n",
      "#559 Loss: 0.0004405444487929344\n",
      "#560 Loss: 0.00044018030166625977\n",
      "#561 Loss: 0.00043981726048514247\n",
      "#562 Loss: 0.0004394551506265998\n",
      "#563 Loss: 0.0004390942631289363\n",
      "#564 Loss: 0.00043873427785001695\n",
      "#565 Loss: 0.00043837571865879\n",
      "#566 Loss: 0.00043801794527098536\n",
      "#567 Loss: 0.0004376611905172467\n",
      "#568 Loss: 0.0004373055708128959\n",
      "#569 Loss: 0.0004369509406387806\n",
      "#570 Loss: 0.00043659756192937493\n",
      "#571 Loss: 0.0004362449690233916\n",
      "#572 Loss: 0.0004358936275821179\n",
      "#573 Loss: 0.000435543101048097\n",
      "#574 Loss: 0.00043519368045963347\n",
      "#575 Loss: 0.0004348453367128968\n",
      "#576 Loss: 0.00043449801160022616\n",
      "#577 Loss: 0.00043415173422545195\n",
      "#578 Loss: 0.00043380618444643915\n",
      "#579 Loss: 0.000433462206274271\n",
      "#580 Loss: 0.00043311883928254247\n",
      "#581 Loss: 0.0004327765782363713\n",
      "#582 Loss: 0.0004324353358242661\n",
      "#583 Loss: 0.00043209505383856595\n",
      "#584 Loss: 0.00043175587779842317\n",
      "#585 Loss: 0.0004314175748731941\n",
      "#586 Loss: 0.0004310802323743701\n",
      "#587 Loss: 0.00043074385030195117\n",
      "#588 Loss: 0.00043040854507125914\n",
      "#589 Loss: 0.00043007422937080264\n",
      "#590 Loss: 0.00042974069947376847\n",
      "#591 Loss: 0.0004294083046261221\n",
      "#592 Loss: 0.0004290769575163722\n",
      "#593 Loss: 0.00042874645441770554\n",
      "#594 Loss: 0.00042841682443395257\n",
      "#595 Loss: 0.0004280882712919265\n",
      "#596 Loss: 0.00042776064947247505\n",
      "#597 Loss: 0.0004274339007679373\n",
      "#598 Loss: 0.0004271082580089569\n",
      "#599 Loss: 0.00042678354657255113\n",
      "#600 Loss: 0.0004264596209395677\n",
      "#601 Loss: 0.0004261365975253284\n",
      "#602 Loss: 0.0004258147091604769\n",
      "#603 Loss: 0.0004254937230143696\n",
      "#604 Loss: 0.00042517358087934554\n",
      "#605 Loss: 0.0004248542827554047\n",
      "#606 Loss: 0.00042453614878468215\n",
      "#607 Loss: 0.0004242186259943992\n",
      "#608 Loss: 0.00042390215094201267\n",
      "#609 Loss: 0.00042358675273135304\n",
      "#610 Loss: 0.000423271965701133\n",
      "#611 Loss: 0.00042295828461647034\n",
      "#612 Loss: 0.00042264547664672136\n",
      "#613 Loss: 0.00042233357089571655\n",
      "#614 Loss: 0.0004220225673634559\n",
      "#615 Loss: 0.00042171234963461757\n",
      "#616 Loss: 0.0004214028303977102\n",
      "#617 Loss: 0.00042109459172934294\n",
      "#618 Loss: 0.0004207869933452457\n",
      "#619 Loss: 0.00042048038449138403\n",
      "#620 Loss: 0.0004201746196486056\n",
      "#621 Loss: 0.00041986966971307993\n",
      "#622 Loss: 0.0004195656511001289\n",
      "#623 Loss: 0.0004192624764982611\n",
      "#624 Loss: 0.00041896029142662883\n",
      "#625 Loss: 0.00041865886305458844\n",
      "#626 Loss: 0.0004183582204859704\n",
      "#627 Loss: 0.000418058451032266\n",
      "#628 Loss: 0.0004177596711087972\n",
      "#629 Loss: 0.0004174615896772593\n",
      "#630 Loss: 0.00041716458508744836\n",
      "#631 Loss: 0.0004168680461589247\n",
      "#632 Loss: 0.00041657270048744977\n",
      "#633 Loss: 0.0004162779659964144\n",
      "#634 Loss: 0.0004159840755164623\n",
      "#635 Loss: 0.00041569105815142393\n",
      "#636 Loss: 0.0004153987974859774\n",
      "#637 Loss: 0.0004151074099354446\n",
      "#638 Loss: 0.0004148166044615209\n",
      "#639 Loss: 0.0004145270795561373\n",
      "#640 Loss: 0.00041423816583119333\n",
      "#641 Loss: 0.0004139501543249935\n",
      "#642 Loss: 0.00041366269579157233\n",
      "#643 Loss: 0.0004133762267883867\n",
      "#644 Loss: 0.0004130906309001148\n",
      "#645 Loss: 0.0004128055879846215\n",
      "#646 Loss: 0.00041252170922234654\n",
      "#647 Loss: 0.00041223838343285024\n",
      "#648 Loss: 0.000411956018069759\n",
      "#649 Loss: 0.0004116740601602942\n",
      "#650 Loss: 0.0004113934701308608\n",
      "#651 Loss: 0.000411113171139732\n",
      "#652 Loss: 0.00041083397809416056\n",
      "#653 Loss: 0.0004105551925022155\n",
      "#654 Loss: 0.0004102775128558278\n",
      "#655 Loss: 0.0004100002988707274\n",
      "#656 Loss: 0.00040972427814267576\n",
      "#657 Loss: 0.00040944854845292866\n",
      "#658 Loss: 0.0004091740702278912\n",
      "#659 Loss: 0.0004089000285603106\n",
      "#660 Loss: 0.00040862683090381324\n",
      "#661 Loss: 0.00040835438994690776\n",
      "#662 Loss: 0.0004080826765857637\n",
      "#663 Loss: 0.0004078118654433638\n",
      "#664 Loss: 0.000407541636377573\n",
      "#665 Loss: 0.0004072721640113741\n",
      "#666 Loss: 0.000407003506552428\n",
      "#667 Loss: 0.0004067356057930738\n",
      "#668 Loss: 0.0004064682871103287\n",
      "#669 Loss: 0.00040620187064632773\n",
      "#670 Loss: 0.0004059361817780882\n",
      "#671 Loss: 0.0004056713660247624\n",
      "#672 Loss: 0.00040540695772506297\n",
      "#673 Loss: 0.00040514362626709044\n",
      "#674 Loss: 0.0004048807022627443\n",
      "#675 Loss: 0.0004046185640618205\n",
      "#676 Loss: 0.00040435741539113224\n",
      "#677 Loss: 0.0004040964995510876\n",
      "#678 Loss: 0.00040383663144893944\n",
      "#679 Loss: 0.00040357731631956995\n",
      "#680 Loss: 0.0004033189034089446\n",
      "#681 Loss: 0.0004030612180940807\n",
      "#682 Loss: 0.0004028039984405041\n",
      "#683 Loss: 0.00040254759369418025\n",
      "#684 Loss: 0.00040229197475127876\n",
      "#685 Loss: 0.0004020369960926473\n",
      "#686 Loss: 0.0004017826577182859\n",
      "#687 Loss: 0.0004015290178358555\n",
      "#688 Loss: 0.00040127610554918647\n",
      "#689 Loss: 0.0004010239790659398\n",
      "#690 Loss: 0.0004007724637631327\n",
      "#691 Loss: 0.00040052185067906976\n",
      "#692 Loss: 0.00040027144132182\n",
      "#693 Loss: 0.0004000221088062972\n",
      "#694 Loss: 0.00039977338747121394\n",
      "#695 Loss: 0.00039952510269358754\n",
      "#696 Loss: 0.0003992776619270444\n",
      "#697 Loss: 0.0003990309196524322\n",
      "#698 Loss: 0.00039878481766209006\n",
      "#699 Loss: 0.00039853944326750934\n",
      "#700 Loss: 0.00039829450543038547\n",
      "#701 Loss: 0.0003980504407081753\n",
      "#702 Loss: 0.0003978069871664047\n",
      "#703 Loss: 0.0003975642321165651\n",
      "#704 Loss: 0.00039732205914333463\n",
      "#705 Loss: 0.00039708061376586556\n",
      "#706 Loss: 0.00039683966315351427\n",
      "#707 Loss: 0.0003965996438637376\n",
      "#708 Loss: 0.00039636020665057003\n",
      "#709 Loss: 0.0003961211768910289\n",
      "#710 Loss: 0.00039588293293491006\n",
      "#711 Loss: 0.0003956453874707222\n",
      "#712 Loss: 0.00039540856960229576\n",
      "#713 Loss: 0.0003951721591874957\n",
      "#714 Loss: 0.00039493650547228754\n",
      "#715 Loss: 0.00039470146293751895\n",
      "#716 Loss: 0.0003944668860640377\n",
      "#717 Loss: 0.0003942333278246224\n",
      "#718 Loss: 0.00039400020614266396\n",
      "#719 Loss: 0.00039376752101816237\n",
      "#720 Loss: 0.00039353559259325266\n",
      "#721 Loss: 0.0003933042171411216\n",
      "#722 Loss: 0.00039307368570007384\n",
      "#723 Loss: 0.0003928436490241438\n",
      "#724 Loss: 0.00039261410711333156\n",
      "#725 Loss: 0.0003923853801097721\n",
      "#726 Loss: 0.0003921570023521781\n",
      "#727 Loss: 0.0003919293521903455\n",
      "#728 Loss: 0.00039170231320895255\n",
      "#729 Loss: 0.000391476001823321\n",
      "#730 Loss: 0.000391250301618129\n",
      "#731 Loss: 0.0003910250379703939\n",
      "#732 Loss: 0.0003908003563992679\n",
      "#733 Loss: 0.0003905765188392252\n",
      "#734 Loss: 0.00039035305962897837\n",
      "#735 Loss: 0.0003901301824953407\n",
      "#736 Loss: 0.0003899080620612949\n",
      "#737 Loss: 0.0003896863490808755\n",
      "#738 Loss: 0.00038946527638472617\n",
      "#739 Loss: 0.0003892448148690164\n",
      "#740 Loss: 0.00038902502274140716\n",
      "#741 Loss: 0.00038880555075593293\n",
      "#742 Loss: 0.0003885868936777115\n",
      "#743 Loss: 0.00038836890598759055\n",
      "#744 Loss: 0.00038815103471279144\n",
      "#745 Loss: 0.00038793415296822786\n",
      "#746 Loss: 0.0003877176495734602\n",
      "#747 Loss: 0.00038750190287828445\n",
      "#748 Loss: 0.00038728670915588737\n",
      "#749 Loss: 0.00038707192288711667\n",
      "#750 Loss: 0.00038685795152559876\n",
      "#751 Loss: 0.0003866442129947245\n",
      "#752 Loss: 0.0003864312020596117\n",
      "#753 Loss: 0.00038621871499344707\n",
      "#754 Loss: 0.0003860069264192134\n",
      "#755 Loss: 0.00038579534157179296\n",
      "#756 Loss: 0.00038558474625460804\n",
      "#757 Loss: 0.0003853743546642363\n",
      "#758 Loss: 0.0003851648070849478\n",
      "#759 Loss: 0.0003849556087516248\n",
      "#760 Loss: 0.0003847471089102328\n",
      "#761 Loss: 0.00038453907473012805\n",
      "#762 Loss: 0.00038433147710748017\n",
      "#763 Loss: 0.0003841245488729328\n",
      "#764 Loss: 0.00038391805719584227\n",
      "#765 Loss: 0.0003837121475953609\n",
      "#766 Loss: 0.0003835069655906409\n",
      "#767 Loss: 0.00038330204552039504\n",
      "#768 Loss: 0.0003830977657344192\n",
      "#769 Loss: 0.00038289400981739163\n",
      "#770 Loss: 0.0003826907486654818\n",
      "#771 Loss: 0.00038248798227868974\n",
      "#772 Loss: 0.00038228597259148955\n",
      "#773 Loss: 0.0003820842830464244\n",
      "#774 Loss: 0.00038188311737030745\n",
      "#775 Loss: 0.0003816825628746301\n",
      "#776 Loss: 0.0003814825031440705\n",
      "#777 Loss: 0.00038128287997096777\n",
      "#778 Loss: 0.00038108378066681325\n",
      "#779 Loss: 0.0003808854380622506\n",
      "#780 Loss: 0.0003806874738074839\n",
      "#781 Loss: 0.0003804898587986827\n",
      "#782 Loss: 0.0003802929713856429\n",
      "#783 Loss: 0.0003800964041147381\n",
      "#784 Loss: 0.0003799004771281034\n",
      "#785 Loss: 0.0003797052486333996\n",
      "#786 Loss: 0.0003795101947616786\n",
      "#787 Loss: 0.00037931554834358394\n",
      "#788 Loss: 0.00037912148400209844\n",
      "#789 Loss: 0.0003789281181525439\n",
      "#790 Loss: 0.0003787352761719376\n",
      "#791 Loss: 0.00037854243419133127\n",
      "#792 Loss: 0.00037835066905245185\n",
      "#793 Loss: 0.00037815922405570745\n",
      "#794 Loss: 0.00037796812830492854\n",
      "#795 Loss: 0.0003777775273192674\n",
      "#796 Loss: 0.00037758779944851995\n",
      "#797 Loss: 0.00037739804247394204\n",
      "#798 Loss: 0.0003772091004066169\n",
      "#799 Loss: 0.0003770206531044096\n",
      "#800 Loss: 0.00037683252594433725\n",
      "#801 Loss: 0.00037664492265321314\n",
      "#802 Loss: 0.00037645793054252863\n",
      "#803 Loss: 0.0003762712294701487\n",
      "#804 Loss: 0.0003760850813705474\n",
      "#805 Loss: 0.00037589939893223345\n",
      "#806 Loss: 0.00037571394932456315\n",
      "#807 Loss: 0.00037552943103946745\n",
      "#808 Loss: 0.0003753453493118286\n",
      "#809 Loss: 0.00037516147131100297\n",
      "#810 Loss: 0.00037497805897146463\n",
      "#811 Loss: 0.00037479508318938315\n",
      "#812 Loss: 0.0003746128058992326\n",
      "#813 Loss: 0.0003744309942703694\n",
      "#814 Loss: 0.0003742495027836412\n",
      "#815 Loss: 0.0003740684478543699\n",
      "#816 Loss: 0.0003738878294825554\n",
      "#817 Loss: 0.0003737077349796891\n",
      "#818 Loss: 0.0003735279606189579\n",
      "#819 Loss: 0.00037334891385398805\n",
      "#820 Loss: 0.0003731701581273228\n",
      "#821 Loss: 0.0003729918971657753\n",
      "#822 Loss: 0.00037281401455402374\n",
      "#823 Loss: 0.0003726367140188813\n",
      "#824 Loss: 0.000372459675418213\n",
      "#825 Loss: 0.00037228333530947566\n",
      "#826 Loss: 0.0003721072862390429\n",
      "#827 Loss: 0.0003719315864145756\n",
      "#828 Loss: 0.00037175643956288695\n",
      "#829 Loss: 0.00037158187478780746\n",
      "#830 Loss: 0.00037140765925869346\n",
      "#831 Loss: 0.00037123364745639265\n",
      "#832 Loss: 0.00037106015952304006\n",
      "#833 Loss: 0.00037088728277012706\n",
      "#834 Loss: 0.00037071475526317954\n",
      "#835 Loss: 0.0003705425187945366\n",
      "#836 Loss: 0.0003703708644025028\n",
      "#837 Loss: 0.0003701997920870781\n",
      "#838 Loss: 0.0003700288652908057\n",
      "#839 Loss: 0.00036985857877880335\n",
      "#840 Loss: 0.00036968867061659694\n",
      "#841 Loss: 0.000369519111700356\n",
      "#842 Loss: 0.00036934996023774147\n",
      "#843 Loss: 0.0003691810998134315\n",
      "#844 Loss: 0.0003690126759465784\n",
      "#845 Loss: 0.0003688449796754867\n",
      "#846 Loss: 0.00036867771996185184\n",
      "#847 Loss: 0.00036851054755970836\n",
      "#848 Loss: 0.00036834398633800447\n",
      "#849 Loss: 0.00036817786167375743\n",
      "#850 Loss: 0.00036801202804781497\n",
      "#851 Loss: 0.0003678466600831598\n",
      "#852 Loss: 0.0003676819324027747\n",
      "#853 Loss: 0.0003675172629300505\n",
      "#854 Loss: 0.00036735317553393543\n",
      "#855 Loss: 0.0003671894082799554\n",
      "#856 Loss: 0.00036702610668726265\n",
      "#857 Loss: 0.0003668632125481963\n",
      "#858 Loss: 0.0003667005221359432\n",
      "#859 Loss: 0.00036653861752711236\n",
      "#860 Loss: 0.0003663768875412643\n",
      "#861 Loss: 0.00036621553590521216\n",
      "#862 Loss: 0.00036605464993044734\n",
      "#863 Loss: 0.0003658942587208003\n",
      "#864 Loss: 0.00036573412944562733\n",
      "#865 Loss: 0.0003655743203125894\n",
      "#866 Loss: 0.00036541506415233016\n",
      "#867 Loss: 0.000365256069926545\n",
      "#868 Loss: 0.000365097657777369\n",
      "#869 Loss: 0.00036493942025117576\n",
      "#870 Loss: 0.0003647817356977612\n",
      "#871 Loss: 0.00036462428397499025\n",
      "#872 Loss: 0.0003644674434326589\n",
      "#873 Loss: 0.00036431089392863214\n",
      "#874 Loss: 0.0003641546063590795\n",
      "#875 Loss: 0.0003639989881776273\n",
      "#876 Loss: 0.0003638433408923447\n",
      "#877 Loss: 0.0003636883047875017\n",
      "#878 Loss: 0.0003635335306171328\n",
      "#879 Loss: 0.0003633794258348644\n",
      "#880 Loss: 0.0003632255538832396\n",
      "#881 Loss: 0.00036307182745076716\n",
      "#882 Loss: 0.00036291865399107337\n",
      "#883 Loss: 0.00036276571336202323\n",
      "#884 Loss: 0.0003626132383942604\n",
      "#885 Loss: 0.00036246117088012397\n",
      "#886 Loss: 0.0003623096563387662\n",
      "#887 Loss: 0.0003621583164203912\n",
      "#888 Loss: 0.00036200747126713395\n",
      "#889 Loss: 0.00036185680073685944\n",
      "#890 Loss: 0.0003617066831793636\n",
      "#891 Loss: 0.00036155691486783326\n",
      "#892 Loss: 0.0003614073502831161\n",
      "#893 Loss: 0.0003612582804635167\n",
      "#894 Loss: 0.00036110947257839143\n",
      "#895 Loss: 0.00036096107214689255\n",
      "#896 Loss: 0.0003608128463383764\n",
      "#897 Loss: 0.00036066528991796076\n",
      "#898 Loss: 0.0003605179372243583\n",
      "#899 Loss: 0.00036037087556906044\n",
      "#900 Loss: 0.0003602243959903717\n",
      "#901 Loss: 0.00036007812013849616\n",
      "#902 Loss: 0.00035993216442875564\n",
      "#903 Loss: 0.0003597865579649806\n",
      "#904 Loss: 0.00035964156268164515\n",
      "#905 Loss: 0.0003594966547098011\n",
      "#906 Loss: 0.0003593523579183966\n",
      "#907 Loss: 0.00035920803202316165\n",
      "#908 Loss: 0.0003590641135815531\n",
      "#909 Loss: 0.0003589208354242146\n",
      "#910 Loss: 0.00035877764457836747\n",
      "#911 Loss: 0.00035863497760146856\n",
      "#912 Loss: 0.00035849251435138285\n",
      "#913 Loss: 0.00035835051676258445\n",
      "#914 Loss: 0.00035820872290059924\n",
      "#915 Loss: 0.0003580672782845795\n",
      "#916 Loss: 0.00035792627022601664\n",
      "#917 Loss: 0.0003577855241019279\n",
      "#918 Loss: 0.00035764501080848277\n",
      "#919 Loss: 0.0003575050795916468\n",
      "#920 Loss: 0.00035736532299779356\n",
      "#921 Loss: 0.00035722600296139717\n",
      "#922 Loss: 0.00035708682844415307\n",
      "#923 Loss: 0.0003569481195881963\n",
      "#924 Loss: 0.00035680975997820497\n",
      "#925 Loss: 0.00035667174961417913\n",
      "#926 Loss: 0.00035653405939228833\n",
      "#927 Loss: 0.00035639668931253254\n",
      "#928 Loss: 0.00035625958116725087\n",
      "#929 Loss: 0.0003561229386832565\n",
      "#930 Loss: 0.00035598649992607534\n",
      "#931 Loss: 0.00035585035220719874\n",
      "#932 Loss: 0.0003557145537342876\n",
      "#933 Loss: 0.0003555791627150029\n",
      "#934 Loss: 0.00035544406273402274\n",
      "#935 Loss: 0.0003553091373760253\n",
      "#936 Loss: 0.0003551748231984675\n",
      "#937 Loss: 0.0003550404799170792\n",
      "#938 Loss: 0.00035490660229697824\n",
      "#939 Loss: 0.0003547731030266732\n",
      "#940 Loss: 0.00035463980748318136\n",
      "#941 Loss: 0.0003545070067048073\n",
      "#942 Loss: 0.0003543744678609073\n",
      "#943 Loss: 0.00035424213274382055\n",
      "#944 Loss: 0.0003541101177688688\n",
      "#945 Loss: 0.00035397845203988254\n",
      "#946 Loss: 0.0003538471064530313\n",
      "#947 Loss: 0.00035371611011214554\n",
      "#948 Loss: 0.0003535854339133948\n",
      "#949 Loss: 0.00035345516516827047\n",
      "#950 Loss: 0.0003533248382154852\n",
      "#951 Loss: 0.0003531950933393091\n",
      "#952 Loss: 0.00035306569770909846\n",
      "#953 Loss: 0.0003529363020788878\n",
      "#954 Loss: 0.0003528074303176254\n",
      "#955 Loss: 0.00035267890780232847\n",
      "#956 Loss: 0.000352550734532997\n",
      "#957 Loss: 0.00035242256126366556\n",
      "#958 Loss: 0.00035229502827860415\n",
      "#959 Loss: 0.0003521676699165255\n",
      "#960 Loss: 0.00035204048617742956\n",
      "#961 Loss: 0.0003519136516842991\n",
      "#962 Loss: 0.0003517870791256428\n",
      "#963 Loss: 0.0003516610595397651\n",
      "#964 Loss: 0.0003515350981615484\n",
      "#965 Loss: 0.0003514094860292971\n",
      "#966 Loss: 0.0003512839903123677\n",
      "#967 Loss: 0.0003511590766720474\n",
      "#968 Loss: 0.0003510344249662012\n",
      "#969 Loss: 0.0003509099769871682\n",
      "#970 Loss: 0.000350785645423457\n",
      "#971 Loss: 0.0003506619541440159\n",
      "#972 Loss: 0.00035053843748755753\n",
      "#973 Loss: 0.00035041497903876007\n",
      "#974 Loss: 0.00035029210266657174\n",
      "#975 Loss: 0.0003501693136058748\n",
      "#976 Loss: 0.00035004675737582147\n",
      "#977 Loss: 0.0003499248414300382\n",
      "#978 Loss: 0.0003498028381727636\n",
      "#979 Loss: 0.0003496813587844372\n",
      "#980 Loss: 0.0003495598793961108\n",
      "#981 Loss: 0.00034943889477290213\n",
      "#982 Loss: 0.00034931814298033714\n",
      "#983 Loss: 0.0003491977113299072\n",
      "#984 Loss: 0.00034907751251012087\n",
      "#985 Loss: 0.0003489576920401305\n",
      "#986 Loss: 0.0003488380752969533\n",
      "#987 Loss: 0.0003487186913844198\n",
      "#988 Loss: 0.00034859933657571673\n",
      "#989 Loss: 0.00034848094219341874\n",
      "#990 Loss: 0.00034836214035749435\n",
      "#991 Loss: 0.00034824394970200956\n",
      "#992 Loss: 0.0003481258754618466\n",
      "#993 Loss: 0.00034800832509063184\n",
      "#994 Loss: 0.00034789080382324755\n",
      "#995 Loss: 0.0003477735153865069\n",
      "#996 Loss: 0.00034765666350722313\n",
      "#997 Loss: 0.0003475398989394307\n",
      "#998 Loss: 0.00034742362913675606\n",
      "#999 Loss: 0.00034730759216472507\n",
      "Predict data based on trained weights: \n",
      "Input: \n",
      "tensor([1.0000, 0.0969, 0.3613])\n",
      "Output: \n",
      "tensor([0.2023])\n"
     ]
    }
   ],
   "source": [
    "NN = FFNN()\n",
    "\n",
    "epochs = 1000 # trains the NN 1,000 times\n",
    "for i in range(epochs):\n",
    "    # print mean sum squared loss\n",
    "    print(\"#\" + str(i) + \" Loss: \" + str(torch.mean(((y - NN(X)) ** 2)).detach().item()))\n",
    "\n",
    "    # training with learning rate = 0.1\n",
    "    NN.train(X, y, 0.1)\n",
    "\n",
    "# save weights\n",
    "SaveModel(NN, \"FFNN\")\n",
    "\n",
    "# load saved weights\n",
    "result = LoadModel(\"FFNN\")\n",
    "\n",
    "# predict x input\n",
    "result.predict(x_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "960f2ae499b536958d588fcafb5f9e40dd995fc898e4b79db932caf22756042a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
